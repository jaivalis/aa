\documentclass[11pt]{article}
\usepackage{array}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgfplotstable}
\usepackage{pgfplots}
\usepackage{filecontents}



\title{
	\textbf{Autonomous Agents Assignment 2}
}

\author{Tobias Stahl \\ 10528199 \and Spyros Michaelides \\ 10523316 \and Ioannis Giounous Aivalis \\ 10524851 \and Francesco Stablum \\ 6200982}




\begin{document}

\maketitle


\section{Introduction}
This report is based on the implementation of an assignment exercise in which a predator is trying to catch a prey in a 2-dimensional environment, being unknown to the predator. This will be attempted  using Temporal Difference learning methods (TD), a combination of Monte Carlo and dynamic programming. TD learning methods can learn directly from experience, rather than having to rely on a model of the environment. TD methods can update action state values estimates based on other learned estimates, without having to wait for the final outcome. 

\section{Algorithms}
In this part of the report the used algorithms are introduced on the basis of their Pseudocode description.


\subsection{Q-Learning}
The initial exercise of this lab assignment is to implement Q-learning, a temporal-difference (TD) learning algorithm. This algorithm is to be used by the predator agent to catch the prey.\\
Q-learning is an off-policy TD control algorithm. An off-policy TD algorithm is one in which the estimated value functions can be updated using hypothetical actions, without having actually executed the actions themselves. Using this approach the algorithm can separate exploration from control, since the deterministic learned policy is separate to the exploration, meaning the agent could learn through the environment without necessarily having had the explicit experience.
The steps involved can be summarised into the following Algorithm ~\ref{algQ}\\\\


\begin{algorithm}
\caption{Q-learning}
\begin{algorithmic}[1]
\label{algQ}
\STATE Initialise $Q(s,a)$ arbitrarily
\FORALL{Episodes}
\STATE Initialise s
\FORALL{Steps of episodes}
\STATE Choose $a$ from $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
\STATE Take action $a$, observe $r$, $s'$
\STATE $Q(s,a) \leftarrow Q(s,a) + \alpha \lbrack r + \gamma max_{a'} Q(s',a') - Q(s,a)\rbrack$
\STATE $s \leftarrow s';$\
\ENDFOR
\STATE Until s is terminal
\ENDFOR
\end{algorithmic}
\end{algorithm}

\noindent Where:
\begin{itemize}
	\item $\alpha$ is the learning rate. Setting it to a high value will force learning to occur faster, whereas for a low value it will occur slower.
	\item $max_{a'}$ is the maximum reward reachable in the state $s'$
	\item $\gamma$ is the value which gives future rewards less worth than immediate ones
\end{itemize}
\vspace*{10mm}


\subsection{Q-learning using $\epsilon$-greedy}

When the Q-learning algorithm to selects an action, there needs to be some form of trade-off between selecting the action with the highest estimated reward so far, and the the rest of the actions available. Limiting the action selection policy to only the best action learned so far, would mean potentially losing out  on a better action in the future, being in a given state. To satisfy this trade-off, $\epsilon$-greedy policy uses a (in this example a small) probability of $\epsilon$ to select randomly between all  between all the actions available in a given state, excluding the most optimal one so far. In turn, (as in this scenario $\epsilon$ is small), the most optimal action is chosen with a much larger probability, $1-\epsilon$, most of the time, giving the policy a tendency to exploit the best action so far most of the time, but not lose out on potentially better actions, which could be found through exploration of the other actions. 



Based on the task at hand, the predator should directly learn a high reward policy without learning a model, since the agent is not supposed to know not know the transition probabilities, nor the reward structure of the environment. It is assumed that convergence should occur as long as all state action pair values continue to be updated using a certain policy (in this case $\epsilon$-greedy. 





\subsection{Softmax Action-Selection Policy}

In this section, a different action selection policy will be used in Q-learning instead of $\epsilon$-greedy. $\epsilon$-greedy policy satisfies the exploration/exploitation variance which is desired to be used in the Q-learning algorithm, although the way in which it achieves this could be a disadvantage in scenarios where the least favourable action has a much worse pay-off than the e.g., the second-best one. When the algorithm explores with a probability $\epsilon$, it does not do this by taking into consideration the performance of the individual non-best actions themselves. The probability distribution between which non-best action is selected, is uniformly distributed and therefore, each one is as likely to be chosen as the rest. To optimise the performance of the Q-learning algorithm, it could be more efficient to make the selection among the non-best actions by weighing them according to their action-value estimates, thus increasing probability of selection for the higher action-value estimates, and hence making a more guided (by value) exploration.
Algorithm ~\ref{softmax} differs from $\epsilon$-greedy in Operation ~\ref{soft}.

\begin{algorithm}
\caption{Softmax}
\begin{algorithmic}[1]
\label{softmax}
\STATE Initialise $Q(s,a)$ arbitrarily
\FORALL{Episodes}
\STATE Initialise s
\FORALL{Steps of episodes}
\STATE Choose Choose $a$ with probability $\frac{\exp^{Q_t(a)/\tau}}{\sum{\exp^{Q_t(b)/\tau}}}$ \label{soft}
\STATE Take action $a$, observe $r$, $s'$
\STATE $Q(s,a) \leftarrow Q(s,a) + \alpha \lbrack r + \gamma max_{a'} Q(s',a') - Q(s,a)\rbrack$
\STATE $s \leftarrow s';$\
\ENDFOR
\STATE Until s is terminal
\ENDFOR
\end{algorithmic}
\end{algorithm}

\noindent Where:
\begin{itemize}
	\item $\tau$ is a positive parameter called \emph{temperature}. A high temperature leads the actions to be almost equiproble, low tempreature values cause a greater difference with $\tau$ $\rightarrow$ 0 beeing the same as greedy action selection.
\end{itemize}
\vspace*{10mm}


\subsection{Sarsa}
Sarsa, like Q-learning is a temporal difference algorithm, meaning that it compares temporally successive predictions. Unlike Q-learning though, Sarsa is an on-policy TD method. In on-policy TD learning the algorithm learns the value of the policy that is used to make the decisions, meaning directly through experience. This is in contrast to off-policy where value functions are not updated solely on experienced actions. The action selection policies previously discussed in Q-learning are also applicable for use in Sarsa. Again, there is the choice of specifying the trade-off between exploitation/exploration by setting the $\epsilon$ parameter when using $\epsilon$-greedy, or using the softmax policy.

\begin{algorithm}
\caption{Sarsa}
\begin{algorithmic}[1]
\label{sarsa}
\STATE Initialise $Q(s,a)$ arbitrarily
\FORALL{Episodes}
\STATE Initialise s
\STATE Choose $a$ from $s$ using derived from $Q$ (e.g., $\epsilon$-greedy)
\FORALL{Steps of episodes}
\STATE Take action $a$, observe $r$, $s'$
\STATE Choose $a'$ from $s'$ using policy derived from Q (e.g., $\epsilon$-greedy)
\STATE $Q(s,a) \leftarrow Q(s,a) + \alpha \lbrack r + \gamma Q(s',a') - Q(s,a)\rbrack$
\STATE $s \leftarrow s';$ $a \leftarrow a';$
\ENDFOR
\STATE Until s is terminal
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Experiments}
This section describes the properties of the system the experiments were tested on and give an overview of the achieved results including plots to visualize them.

%--2) is there anything to mention about the implementation/machinery the reader should know? (e.g. "these experiments were performed on a [insert machine specs]" when presenting runtimes)
\subsection{System Properties}
The experiments were performend on a 

\subsection{Experiment 1}

%--1) why? "in order to test..." (questions, and preferably hypotheses with explanation)
\subsubsection{Hypotheses}
The first experiment aims to measure the performance of the predator catching the predator with different learning rates $\alpha$ and different discount factors $\gamma$ over time. Therefore the average performance of 100 simulations with different $\alpha$ and $\gamma$ for learning with an episode count from 50 to 950 is  taken into account. The reduced state space  with 11x11 states is chosen, in order to save computational time.\\
The expected outcome is that high learning rates tend to always replace the current value with the new estimates and converge quickly, while a small learning rate value leads to a slow convergence and seems to trust the current estimate. ~\cite{dar}

\subsubsection{Results}
The results of this experiment are plottet in Table ~\ref{plotsEx1}. The five chosen learning rates $\alpha$ are shown in each of the four graphs with different discount factors $\gamma$.\\
The interpretation of these results will follow in the next section.

\begin{table}
\begin{tabular}{cc}
$\gamma$ = 0.1 & $\gamma$ = 0.5\\
\resizebox{200pt}{!}{
\begin{tikzpicture}
\begin{axis}[xlabel={Learned Episodes},ylabel={Average Performance}, legend style={at={(1.2,1)},anchor=north,legend cell align=left}]


\addplot table[x index=0,y index=1,col sep=comma] {epsGreedGamma1.dat};
\addlegendentry{$\alpha$=0.1}

\addplot table[x index=0,y index=2,col sep=comma] {epsGreedGamma1.dat};
\addlegendentry{$\alpha$=0.2}

\addplot table[x index=0,y index=3,col sep=comma] {epsGreedGamma1.dat};
\addlegendentry{$\alpha$=0.3}

\addplot table[x index=0,y index=4,col sep=comma] {epsGreedGamma1.dat};
\addlegendentry{$\alpha$=0.4}

\addplot table[x index=0,y index=5,col sep=comma] {epsGreedGamma1.dat};
\addlegendentry{$\alpha$=0.5}


\end{axis}
\end{tikzpicture}
}
&
\resizebox{200pt}{!}{
\begin{tikzpicture}
\begin{axis}[xlabel={Learned Episodes},ylabel={Average Performance}, legend style={at={(1.2,1)},anchor=north,legend cell align=left}]


\addplot table[x index=0,y index=1,col sep=comma] {epsGreedGamma5.dat};
\addlegendentry{$\alpha$=0.1}

\addplot table[x index=0,y index=2,col sep=comma] {epsGreedGamma5.dat};
\addlegendentry{$\alpha$=0.2}

\addplot table[x index=0,y index=3,col sep=comma] {epsGreedGamma5.dat};
\addlegendentry{$\alpha$=0.3}

\addplot table[x index=0,y index=4,col sep=comma] {epsGreedGamma5.dat};
\addlegendentry{$\alpha$=0.4}

\addplot table[x index=0,y index=5,col sep=comma] {epsGreedGamma5.dat};
\addlegendentry{$\alpha$=0.5}


\end{axis}
\end{tikzpicture}
}\\
$\gamma$ = 0.7 & $\gamma$ = 0.9\\
\resizebox{200pt}{!}{
\begin{tikzpicture}
\begin{axis}[xlabel={Learned Episodes},ylabel={Average Performance}, legend style={at={(1.2,1)},anchor=north,legend cell align=left}]


\addplot table[x index=0,y index=1,col sep=comma] {epsGreedGamma7.dat};
\addlegendentry{$\alpha$=0.1}

\addplot table[x index=0,y index=2,col sep=comma] {epsGreedGamma7.dat};
\addlegendentry{$\alpha$=0.2}

\addplot table[x index=0,y index=3,col sep=comma] {epsGreedGamma7.dat};
\addlegendentry{$\alpha$=0.3}

\addplot table[x index=0,y index=4,col sep=comma] {epsGreedGamma7.dat};
\addlegendentry{$\alpha$=0.4}

\addplot table[x index=0,y index=5,col sep=comma] {epsGreedGamma7.dat};
\addlegendentry{$\alpha$=0.5}


\end{axis}
\end{tikzpicture}
}
&
\resizebox{200pt}{!}{
\begin{tikzpicture}
\begin{axis}[xlabel={Learned Episodes},ylabel={Average Performance}, legend style={at={(1.2,1)},anchor=north,legend cell align=left}]


\addplot table[x index=0,y index=1,col sep=comma] {epsGreedGamma9.dat};
\addlegendentry{$\alpha$=0.1}

\addplot table[x index=0,y index=2,col sep=comma] {epsGreedGamma9.dat};
\addlegendentry{$\alpha$=0.2}

\addplot table[x index=0,y index=3,col sep=comma] {epsGreedGamma9.dat};
\addlegendentry{$\alpha$=0.3}

\addplot table[x index=0,y index=4,col sep=comma] {epsGreedGamma9.dat};
\addlegendentry{$\alpha$=0.4}

\addplot table[x index=0,y index=5,col sep=comma] {epsGreedGamma9.dat};
\addlegendentry{$\alpha$=0.5}


\end{axis}
\end{tikzpicture}
}
\end{tabular}
\caption{Plots show the average performance over time spendt learning the policy with learning rates $\alpha$ from 0.1 to 0.5. The first plot shows the curve for discount factor $\gamma$ = 0.1, second $\gamma$ = 0.5, third $\gamma$ = 0.7 and fourth $\gamma$ = 0.9}
\label{plotsEx1}
\end{table}

%--3) what does it show (and is that what you expected, and why)
\subsubsection{Interpretation}


%--4) take home message (what do you want the reader to remember, e.g. "Therefore, by reducing the state space, we have gained several orders of magnitude in runtime.")
\subsubsection{}



\subsection{Experiment 2}

%--1) why? "in order to test..." (questions, and preferably hypotheses with explanation)
\subsubsection{Hypotheses}
The second experiment observes the impact of $\epsilon$ in the $\epsilon$-greedy action selection policy and the initialization of the Q-table. $\epsilon$ determines the exploration rate, with values close to zero favouring exploitation and values close to 1 prefering exploration.\\
In order to test this, the constant learning rate = 0.8 and the constant discount factor = 0.9 are chosen, since these values had the best performance in the previous experiment. The learning episode count is 1000 episodes and the number of simulations is 100.\\
In the previous experiment the Q-Table was initialized optimistically, with values higher than the actual reward to receive, which encourages an early exploration, since any actual reward is less than the actual reward. Changing this value to a pessimistical initalization on the other hand inspires exploitation.

\subsubsection{Results}



\resizebox{450pt}{!}{
\begin{tikzpicture}
\begin{axis}[xlabel={$\epsilon$},ylabel={Average Length}, legend style={at={(1.3,1)},anchor=north,legend cell align=left}]

\addplot table[x index=0,y index=1,col sep=comma] {epsilonvsqvalue.dat};
\addlegendentry{$Q_0$=30}

\addplot table[x index=0,y index=2,col sep=comma] {epsilonvsqvalue.dat};
\addlegendentry{$Q_0$=25}

\addplot table[x index=0,y index=3,col sep=comma] {epsilonvsqvalue.dat};
\addlegendentry{$Q_0$=20}

\addplot table[x index=0,y index=4,col sep=comma] {epsilonvsqvalue.dat};
\addlegendentry{$Q_0$=15}

\addplot table[x index=0,y index=5,col sep=comma] {epsilonvsqvalue.dat};
\addlegendentry{$Q_0$=10}

\addplot table[x index=0,y index=6,col sep=comma] {epsilonvsqvalue.dat};
\addlegendentry{$Q_0$=5}

\addplot table[x index=0,y index=7,col sep=comma] {epsilonvsqvalue.dat};
\addlegendentry{$Q_0$=0}

\addplot table[x index=0,y index=8,col sep=comma] {epsilonvsqvalue.dat};
\addlegendentry{$Q_0$=-5}

\addplot table[x index=0,y index=9,col sep=comma] {epsilonvsqvalue.dat};
\addlegendentry{$Q_0$=-10}

\addplot table[x index=0,y index=10,col sep=comma] {epsilonvsqvalue.dat};
\addlegendentry{$Q_0$=-15}

\end{axis}
\end{tikzpicture}
}





%--3) what does it show (and is that what you expected, and why)
\subsubsection{Interpretation}


%--4) take home message (what do you want the reader to remember, e.g. "Therefore, by reducing the state space, we have gained several orders of magnitude in runtime.")
\subsubsection{}

\subsection{Experiment 3}

%--1) why? "in order to test..." (questions, and preferably hypotheses with explanation)
\subsubsection{Hypotheses}
In this experiment the difference between the $\epsilon$-greedy and the softmax action selection policy is researched. The $\epsilon$-greedy action selection chooses all actions, except the action with the highest estimated reward so far, with the same probability, while the softmax approach weights the other actions and chooses actions with more pay-off with a higher probability.\\
The experiment setup is the same as in Experiment 1, except that the softmax action selection policy is used instead.\\
Assuming that 
\subsubsection{Results}


\resizebox{450pt}{!}{
\begin{tikzpicture}
\begin{axis}[xlabel={$\alpha$},ylabel={Average Length}, legend style={at={(1.2,1)},anchor=north,legend cell align=left}]


\addplot table[x index=0,y index=2,col sep=comma] {softmax.dat};
\addlegendentry{$\gamma$=0.1}

\addplot table[x index=0,y index=3,col sep=comma] {softmax.dat};
\addlegendentry{$\gamma$=0.2}

\addplot table[x index=0,y index=4,col sep=comma] {softmax.dat};
\addlegendentry{$\gamma$=0.3}

\addplot table[x index=0,y index=5,col sep=comma] {softmax.dat};
\addlegendentry{$\gamma$=0.4}

\addplot table[x index=0,y index=6,col sep=comma] {softmax.dat};
\addlegendentry{$\gamma$=0.5}

\addplot table[x index=0,y index=7,col sep=comma] {softmax.dat};
\addlegendentry{$\gamma$=0.6}

\addplot table[x index=0,y index=8,col sep=comma] {softmax.dat};
\addlegendentry{$\gamma$=0.7}

\addplot table[x index=0,y index=9,col sep=comma] {softmax.dat};
\addlegendentry{$\gamma$=0.8}

\end{axis}
\end{tikzpicture}
}

%--3) what does it show (and is that what you expected, and why)
\subsubsection{Interpretation}


%--4) take home message (what do you want the reader to remember, e.g. "Therefore, by reducing the state space, we have gained several orders of magnitude in runtime.")
\subsubsection{}

\subsection{Experiment 4}

%--1) why? "in order to test..." (questions, and preferably hypotheses with explanation)
\subsubsection{Hypotheses}


\subsubsection{Results}


%--3) what does it show (and is that what you expected, and why)
\subsubsection{Interpretation}


%--4) take home message (what do you want the reader to remember, e.g. "Therefore, by reducing the state space, we have gained several orders of magnitude in runtime.")
\subsubsection{}

%redundance with take home message?
\section{Conclusion}

\begin{thebibliography}{9}

\bibitem{sutton}
  Richard S. Sutton and Andrew G. Barto ,
  \emph{Reinforcement Learning: An Introduction}.
  The MIT Press, Cambridge, Massachusetts

\bibitem{dar}
  Eyal Even-Dar and Yishay Mansour ,
  \emph{Learning Rates for Q-learning}.
  Journal of Machine Learning Research 5 (2003)

\end{thebibliography}

\end{document}