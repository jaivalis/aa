\documentclass[11pt]{article}
\usepackage{array}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}


\title{
	\textbf{Autonomous Agents Assignment 2}
}

\author{Tobias Stahl \\ 10528199 \and Spyros Michaelides \\ 10523316 \and Ioannis Giounous Aivalis \\ 10524851 \and Francesco Stablum \\ 6200982}




\begin{document}

\maketitle




\section{Introduction}



\section{Q-Learning}
The initial exercise of this lab assignment is to implement Q-learning the temporal-difference (TD) learning algorithm. This algorithm is to be used by the predator agent to catch the prey.



\subsection{Summary}
Q-learning is an off-policy TD control algorithm. An off-policy TD algorithm is one in which the estimated value functions can be updated using hypothetical actions, without having actually executed the actions themselves. Using this approach the algorithm can separate exploration from control, meaning the agent could learn through from the environment without necessarily having had the explicit experience.
The steps involved can be summarised into the following:\\\\

\pagebreak

\noindent Initialise $Q(s,a)$ arbitrarily\\
Repeat(for each episode):\\
\hspace*{10mm} Initialise s\\
\hspace*{10mm} Repeat (for each step of episode):\\
\hspace*{20mm} Choose $a$ from $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)\\
\hspace*{20mm} Take action $a$, observe $r$, $s'$\\
\hspace*{20mm} $Q(s,a) \leftarrow Q(s,a) + \alpha \lbrack r + \gamma max_{a'} Q(s',a') - Q(s,a)\rbrack$\\
\hspace*{20mm} $s \leftarrow s';$\\
\hspace*{10mm} until $s$ is terminal\\


Where:
\begin{itemize}
	\item $\alpha$ is the learning rate. Setting it to a high value will force learning to occur faster, whereas for a low value it will occur slower.
	\item $max_{a'}$ is the maximum reward reachable in the state $s'$
	\item $\gamma$ is the value which gives future rewards less worth than immediate ones
\end{itemize}


Which can be translated into the following procedure:
\begin{enumerate}
	\item Initialise the values for \textbf{Q(s,a)}
	\item Observe the current state, \textbf{s}.
	\item Choose an action \textbf{a} for the state based on action selection policy.
	\item Take action, and observe reward \textbf{r}, and new state \textbf{s'}.
	\item Update Q-value for the state \textbf{s} using the observed reward and the maximum reward possible for the next state.
	\item Set the state to the new state, and repeat the process until a state is reached.
\end{enumerate}




\end{document}