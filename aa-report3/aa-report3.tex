\documentclass[a4paper,10pt]{article}
\usepackage{array}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgfplotstable}
\usepackage{pgfplots}
\usepackage{filecontents}
\usepackage{amsmath}
\usepackage{float}



\title{
	\textbf{Autonomous Agents Assignment 3}
}

\author{Tobias Stahl \\ 10528199 \and Spyros Michaelides \\ 10523316 \and Ioannis Giounous Aivalis \\ 10524851 \and Francesco Stablum \\ 6200982}




\begin{document}

\maketitle


\section{Introduction}

%--A general framework for explaining your results/experiments is to follow these points : 

%HYPOTHESIS
%--1) why? "in order to test..." (questions, and preferably hypotheses with explanation) --> Hypothesis
%--2) is there anything to mention about the implementation/machinery the reader should know? (e.g. "these experiments were performed on a [insert machine specs]" when presenting runtimes) - 

%RESULTS

%Interpretation
%--3) what does it show (and is that what you expected, and why)

%Findings
%--4) take home message (what do you want the reader to remember, e.g. "Therefore, by reducing the state space, we have gained several orders of magnitude in runtime.")


\section{New Environment}
%intro, summary
In this assignment, the environment changes, due to the circumstances that for one thing the prey is intelligent and learns how to escape from getting caught by the predator and for another thing there are up to four predators on a team on the board. Additionally the prey and the predators take their actions simultaneously.\\
The rewards of the game change in the following way:
\begin{itemize}
	\item Predator catches prey: Team of predators get a +10 reward and prey gets a -10 reward
	\item Predator runs into other predator: Team of predators get a -10 reward and prey gets a +10 reward
\end{itemize}
The second event only counts if two predators end up on the same square after their actions, but they are allowed to pass through each other without ending up on the same square. If a predator runs into another one, the prey will escape and the episode ends.\\
The last change is the factor that the prey doesn't move randomly anymore, it can take the same actions as the predators, WAIT, EAST, WEST, SOUTH and NORTH, coming along with a tripping probability of 0.2.\\
The state space of the new environment changes as follows \dots

\subsection{Experiment}
The first experiment tests the new implemented environment according to the first simulation with random policies of the first assignment with the difference of the new rewards and the additional predators. The average performance of 100 rounds is evaluated for teams of one, two, three and four predators.

\subsubsection{Hypothesis}
Since both the predators and the prey choose their actions randomly, the predators aren't supposed to find a fast way to catch the prey. Considering that there are more predators on the board, the number of steps of an episode is assumed to be decreased with an increasing number of predators. The decreasing number of steps has its origin not only in the circumstance that more predators are more likely to catch the prey faster, but in the factor that an episode will also end, if two predators end up on the same square, especially considering the starting position of the predators.

\subsection{Results}
%Interpretation
Include figure of average performance of 1,2,3 and 4 predators

%Findings







\section{minimax-Q}
%intro, summary
\subsection{Summary}
The following summary introduces briefly the consequences of using the Markov game framework in place of MDP's in reinforcement learning for two-player zero-sum games.
Since in Markov games the best choice of action depends on the opponent there is no undominated policy, so the problem could be resolved using components from game-theory. Adopting this to the two agent problem, the solution could be approached by evaluating each policy with respect to the scenario where the opponent acts in the worst possible way for the agent, i.e. the opponent tries to minimize the reward function of the agent. Based on the opponent's strategy, the agent will attempt to select the policy that maximizes the discounted reward. Taking into consideration the the action of the opponent as $o \in O$, the value of a state in a Markov game can be redefined as the expected reward for the optimal policy starting from a state $s$, and the quality $Q(s,a,o)$ as the expected reward for taking action $a$ when the opponent chooses $o$, and continuing optimally afterwards. \\
Minimax-Q works in the same way as Q-learning, with the difference that it takes as an action one that minimizes the possible loss for a worst case scenario under the opponent's action o instead of just the maximum approximated action value (without considering the opponents action).
Littman in his paper implements an experiment with two agents 'playing football' on a 4x5 grid, where the aim is for the agent with the ball to move into the correct goal post. Littman implements a series of experiments to compare the performance of minimax-Q in comparison to Q-learning, using different training methods (learning against a random opponent or the algorithm itself.) The results are that minimax-Q allows the agent to converge to a risk averse strategy, due to the fact that it constantly takes into consideration the worst possible action the opponent could take. 


\subsection{Experiment}



\subsubsection{Hypothesis}


\subsection{Results}
%Interpretation


%Findings





\section{Discussion}



\section{Conclusion}


\begin{thebibliography}{9}

\bibitem{sutton}
  Richard S. Sutton and Andrew G. Barto ,
  \emph{Reinforcement Learning: An Introduction}.
  The MIT Press, Cambridge, Massachusetts

\end{thebibliography}

\end{document}
